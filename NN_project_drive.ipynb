{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_project_drive.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1gxKH_X2iSnFhNPDbMTaBFkxwEJYzzw3c",
      "authorship_tag": "ABX9TyMsz/KD7J/xF9alIJw9gjuK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Majd-Wardeh/NN_project_colab/blob/send_to_joyce/NN_project_drive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od2gXpb8pGDs",
        "colab_type": "code",
        "outputId": "4879b625-0b6e-4820-d828-a006698fd8be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-07 13:10:09--  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.15.128, 2a00:1450:400c:c00::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.15.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87910968 (84M) [application/x-hdf]\n",
            "Saving to: ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’\n",
            "\n",
            "/tmp/inception_v3_w 100%[===================>]  83.84M  79.8MB/s    in 1.1s    \n",
            "\n",
            "2020-05-07 13:10:12 (79.8 MB/s) - ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [87910968/87910968]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmXXGNwTpAmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import io\n",
        "import os\n",
        "from os import path\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import scipy\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import backend as k\n",
        "\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbSerjAypBG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(images_dir, pd_file):\n",
        "  data = pd_file \n",
        "  imgs = []\n",
        "  labels = []\n",
        "  points = []\n",
        "  not_found_images_counter = 0\n",
        "  listdir = os.listdir(images_dir)\n",
        "  for index in data.index.values:\n",
        "    image_file_name = 'screen_640x480_{}.jpg'.format(data['pic number'][index])\n",
        "    #if path.exists(images_dir + '/' + image_file_name):\n",
        "    if image_file_name in listdir:\n",
        "      r = data['r'][index]\n",
        "      phai = data['phai'][index]\n",
        "      theta = data['theta'][index]\n",
        "      labels.append([r, phai, theta])\n",
        "      imgs.append(image_file_name)\n",
        "    else:\n",
        "      not_found_images_counter += 1\n",
        "      #print(\"{} not found!\".format(image_file_name))\n",
        "\n",
        "  print(\"found {} images\".format(len(imgs)), \"  # of not found images {}\".format(not_found_images_counter))\n",
        "  return imgs, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPwsQ_291HFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_train_validation_dataframe(csv_file, random_seed=0):\n",
        "  data = pd.read_csv(csv_file)\n",
        "\n",
        "  train_df = data.copy()\n",
        "  valid_df = data.copy()\n",
        "\n",
        "  remove_training = []\n",
        "  remove_validation = []\n",
        "\n",
        "  np.random.seed(random_seed)\n",
        "  for index in data.index.values:\n",
        "    rand = np.random.rand()\n",
        "    if rand < 0.85:\n",
        "      remove_validation.append(index)\n",
        "    else:\n",
        "      remove_training.append(index)\n",
        "\n",
        "  train_df = train_df.drop(remove_training)\n",
        "  valid_df = valid_df.drop(remove_validation)\n",
        "\n",
        "  print(\"total images: \", len(list(data.index.values)))\n",
        "  print(\"number of training images: {}\".format(len(train_df['pic number'])))\n",
        "  print(\"number of validation images: {}\".format(len(valid_df['pic number'])))\n",
        "\n",
        "  # train_df.to_csv(\"/content/drive/My Drive/NN_project/dataset/train_debug.csv\")\n",
        "  return  train_df, valid_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivuEOJH3pfRH",
        "colab_type": "code",
        "outputId": "5668e583-4d9d-4cd1-bceb-c670c68884d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "base_dir = \"/content/drive/My Drive/NN_project/dataset\"\n",
        "train_dir = os.path.join( base_dir, 'images')\n",
        "validation_dir = os.path.join( base_dir, 'images')\n",
        "\n",
        "train_df, valid_df = create_train_validation_dataframe(base_dir + '/coordinates_cleaned_0_11999.csv',\\\n",
        "                                                       random_seed = 7493)\n",
        "\n",
        "train_img_names, train_labels = load_data(base_dir + '/images', train_df)\n",
        "valid_img_names, valid_labels = load_data(base_dir + '/images', valid_df)\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total images:  11996\n",
            "number of training images: 10180\n",
            "number of validation images: 1816\n",
            "found 8982 images   # of not found images 1198\n",
            "found 1609 images   # of not found images 207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVOSqVuxpmn3",
        "colab_type": "code",
        "outputId": "8e7ae60c-6025-455f-9a93-70c4d6e9991c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "inception_weights_dir = '/content/drive/My Drive/NN_project/inception_weights'\n",
        "\n",
        "pre_trained_model = InceptionV3(input_shape = (240, 320, 3), \n",
        "                                include_top = False, \n",
        "                                weights = None)\n",
        "\n",
        "pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "# for layer in pre_trained_model.layers:\n",
        "#   layer.trainable = False\n",
        "  \n",
        "# pre_trained_model.summary()\n",
        "\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "print('last layer output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output\n",
        "\n",
        "\n",
        "flatten = layers.Flatten()(last_output)\n",
        "flatten_dropedout = layers.Dropout(0.3)(flatten)\n",
        "\n",
        "dense1 = layers.Dense(500, activation='relu')(flatten_dropedout)\n",
        "dense1 = layers.Dropout(0.2)(dense1)                  \n",
        " \n",
        "dense2 = layers.Dense(100, activation='relu')(dense1) \n",
        "#dense2 = layers.Dropout(0.2)(dense2)\n",
        "\n",
        "#dense3 = layers.Dense(1024, activation='tanh')(dense2)\n",
        "\n",
        "y_estimator = layers.Dense(3, activation='linear')(dense2)\n",
        "\n",
        "estimation_model = Model( pre_trained_model.input, y_estimator) \n",
        "estimation_model.compile(optimizer = Adam(lr=0.00001),  metrics=['mse'],\n",
        "              loss = 'mse')\n",
        "\n",
        "# estimation_model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "last layer output shape:  (None, 13, 18, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlEgGJw7ppwm",
        "colab_type": "code",
        "outputId": "431e86f4-be15-40ee-e7b7-399b43950130",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import multiprocessing\n",
        "import time\n",
        "\n",
        "class Processor:\n",
        "  def __init__(self, path, img_names, labels):\n",
        "    self.labels = labels\n",
        "    self.img_names = img_names\n",
        "    self.path = path\n",
        "    print(\"hello\")\n",
        "\n",
        "  def __call__(self, index):\n",
        "    #print(index)\n",
        "    img_name = self.img_names[index]\n",
        "    img = Image.open(self.path + \"/\" + img_name) #.resize((320, 240))\n",
        "    img = np.array(img, dtype=np.float32)/255.0\n",
        "    label = self.labels[index]\n",
        "    # label[1] = label[1]*180.0/np.pi\n",
        "    # label[2] = label[2]*180.0/np.pi\n",
        "    return img, label\n",
        "\n",
        "def get_data_multiprocess(data_dir, img_names, labels, length):\n",
        "  start = time.time()\n",
        "  proc=Processor(data_dir, img_names, labels)\n",
        "  pool=multiprocessing.Pool(processes=8)\n",
        "  res = pool.map(proc,range(length))\n",
        "  finish = time.time()\n",
        "  print(\"# of images: \", len(res))\n",
        "  print(\"time = \", finish-start)\n",
        "  x_data = np.empty((length, 240, 320, 3), dtype=np.float32)\n",
        "  y_data = np.empty((length, 3), dtype=np.float32)\n",
        "  for i, r in enumerate(res):\n",
        "    x, y = r\n",
        "    x_data[i, ] = x \n",
        "    y_data[i, ] = y \n",
        "  return x_data, y_data\n",
        "\n",
        "\n",
        "# x_train, y_train = None, None\n",
        "# x_train, y_train = get_data_multiprocess(train_dir, train_img_names, train_labels, int(len(train_labels)))\n",
        "\n",
        "x_valid, y_valid = None, None\n",
        "x_valid, y_valid = get_data_multiprocess(validation_dir, valid_img_names, valid_labels, int(len(valid_img_names)))\n",
        "\n",
        "# print(x_train.shape, y_train.shape)\n",
        "print(x_valid.shape, y_valid.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello\n",
            "# of images:  1609\n",
            "time =  181.07472229003906\n",
            "(1609, 240, 320, 3) (1609, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI2YWvzEmCqL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ddcf74f6-1f32-4205-e0a4-cc33d4ecf956"
      },
      "source": [
        "\n",
        "print(x_valid[0].shape)\n",
        "\n",
        "image = x_valid[0:100]\n",
        "# image = np.expand_dims(image, axis=0)\n",
        "print(image.shape)\n",
        "counter = 0\n",
        "start = time.time()\n",
        "# while( counter < 1000):\n",
        "estimation_model.predict(image)\n",
        "  # counter += 1\n",
        "end = time.time()\n",
        "diff = (end - start)/1.0\n",
        "FPS = 1/diff\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(240, 320, 3)\n",
            "(100, 240, 320, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjad6Fg2UiAe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bcd95e07-0970-40b1-ac36-39aa6a0a8c00"
      },
      "source": [
        "print(FPS)\n",
        "print(1/FPS)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14.700195918310126\n",
            "0.06802630424499512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-kY1AoxpvYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fad =\n",
        "history = estimation_model.fit(x_train, y_train,\n",
        "                    batch_size = 100,\n",
        "                    validation_data = (x_valid, y_valid),                                 \n",
        "                    steps_per_epoch = 5,\n",
        "                    validation_steps = 5,\n",
        "                    epochs=100)\n",
        "\n",
        "# history = estimation_model.fit(x_train, y_train,\n",
        "#                     batch_size = 20,\n",
        "#                     validation_split=0.0,\n",
        "#                     shuffle = False,\n",
        "#                     validation_data = (x_valid, y_valid),                                 \n",
        "#                     steps_per_epoch = 2,\n",
        "#                     validation_steps = 10,\n",
        "#                     epochs=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogUW70JUuwon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "#acc = history.history['accuracy']\n",
        "#val_acc = history.history['val_accuracy']\n",
        "loss = history.history['mse'] #history.history['loss']\n",
        "val_loss = history.history['val_mse'] #history.history['val_loss']\n",
        "start = 2\n",
        "end = 65 #len(loss)\n",
        "loss = loss[start:end]\n",
        "val_loss = val_loss[start:end]\n",
        "epochs = range(len(loss))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs,  val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss (MSE metric)')\n",
        "plt.legend(loc=0)\n",
        "\n",
        "save = True\n",
        "if save:\n",
        "  file_name = \"/MAE_tanh_2layers_500by100_MSE_metric_TR_finetune_{}\".format(time.time())\n",
        "  hist_data = pd.DataFrame(history.history)\n",
        "  # hist_data = hist_data.append()\n",
        "  plt.savefig(base_dir + file_name + \".png\")\n",
        "  hist_data.to_csv(base_dir + file_name + \".csv\")\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G1DEME9fnt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load1_or_save0 = \n",
        "name = \"/content/drive/My Drive/NN_project/inception_weights/weights_MAE_2layers_500each_relu_{}.h5\".format(time.time())\n",
        "if load1_or_save0 == 0:\n",
        "  estimation_model.save_weights(name)\n",
        "else:\n",
        "  estimation_model.load_weights(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PCcMepT6fv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in estimation_model.layers:\n",
        "  layer.trainable = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j4b43GrwAtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = np.random.randint(len(train_labels))\n",
        "y_pred = estimation_model.predict(np.expand_dims(x_train[index], axis=0))[0]\n",
        "y_true = y_train[index]\n",
        "print(y_true)\n",
        "print(y_pred)\n",
        "print(tf.keras.losses.mean_absolute_error(y_true, y_pred), tf.keras.losses.mean_squared_error(y_true, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTq5tVgBwM15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy.linalg as la\n",
        "\n",
        "y_pred = estimation_model.predict(x_train)\n",
        "y_true = y_train\n",
        "\n",
        "l2_norm = la.norm(y_true - y_pred, axis = 1)\n",
        "print(\"norm of the error = \", l2_norm.shape)\n",
        "print(\"mse = \", np.mean(l2_norm))\n",
        "print(\"meidan = \", np.median(l2_norm))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGlKRDSczLzj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp -r \"drive/My Drive/NN_project/dataset/train_7_12/gate/.\" \"drive/My Drive/NN_project/dataset/images/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N9yKLvI2f3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !ls \"drive/My Drive/NN_project/dataset/train_7_12/gate\"\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4oRCqMY1D71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !rm -d -r \"drive/My Drive/NN_project/dataset/images\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V9kqovQzjKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm '/content/drive/My Drive/NN_project/dataset/images/screen_640x480_8197.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYFM9T5V0Xmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !mkdir \"drive/My Drive/NN_project/dataset/images\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blvtUqNKTefd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "\n",
        "# for file_name in os.listdir(\"drive/My Drive/NN_project/dataset/validation_7_12/gate\"):\n",
        "#    list_split = file_name.split('_')\n",
        "#    new_name = list_split[0] + \"_640x480_\" + list_split[2]\n",
        "#    print(file_name, new_name)\n",
        "#    os.rename(\"drive/My Drive/NN_project/dataset/validation_7_12/gate/\" + file_name, \\\n",
        "#              \"drive/My Drive/NN_project/dataset/validation_7_12/gate/\"+ new_name)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}